{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "kLcWJyOiwYHb",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"t-SNE and UMAP on MNIST\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-title: Contents\n",
        "    toc-depth: 4\n",
        "    code-fold: show\n",
        "    self-contained: true\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCIytZDZwYHb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In the previous lab, we learned that PCA performs dimensionality reduction by finding linear combinations of original features. While effective for linear relationships, PCA cannot capture complex non-linear patterns in data.\n",
        "\n",
        "**Kernel PCA** extends PCA to handle non-linear relationships by applying kernel methods. However, it has two main limitations:\n",
        "- Computationally expensive for large datasets\n",
        "- Requires careful selection of kernel type and parameters\n",
        "\n",
        "In this lab, we explore two powerful alternatives for visualizing high-dimensional data:\n",
        "\n",
        "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Excels at preserving local structure and revealing clusters, making it ideal for visualization\n",
        "- **UMAP (Uniform Manifold Approximation and Projection)**: Balances both local and global structure preservation while being computationally efficient\n",
        "\n",
        "We'll apply these methods to the MNIST handwritten digit dataset and compare their performance against PCA and Kernel PCA. Through this comparison, you'll understand the strengths, computational trade-offs, and appropriate use cases for each dimensionality reduction technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrRYANyuwYHc"
      },
      "source": [
        "## Learning Goals\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Compare and contrast different dimensionality reduction approaches (PCA, Kernel PCA, t-SNE, UMAP) using the MNIST dataset\n",
        "* Optimize t-SNE visualizations by tuning key hyperparameters: perplexity, initialization methods, and early exaggeration\n",
        "* Optimize UMAP visualizations by adjusting n_neighbors, min_dist, and distance metrics\n",
        "* Understand the strengths, limitations, and appropriate use cases for each dimensionality reduction method\n",
        "* Make informed decisions about which technique to use based on dataset characteristics and computational constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N5_m4p7wYHc"
      },
      "source": [
        "## MNIST Dataset\n",
        "\n",
        "The MNIST dataset is a collection of 70,000 handwritten digit images, each represented as a 28x28 pixel grayscale image (784 dimensions). This high-dimensional dataset serves as an excellent benchmark for comparing dimensionality reduction techniques, as the underlying structure corresponds to the ten digit classes (0-9). We will visualize how different methods capture and preserve this structure in lower-dimensional spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riN8_7OTwYHc"
      },
      "outputs": [],
      "source": [
        "# import commonly used libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# import the data\n",
        "import sklearn.datasets\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial.distance import squareform, pdist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7y8IONwwYHd",
        "outputId": "dbb4c57f-33af-49f5-90a3-2baa697843fa"
      },
      "outputs": [],
      "source": [
        "#read mnist dataset from sklearn\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784', version=1)\n",
        "\n",
        "# show the shape of the data\n",
        "print(mnist.data.shape)\n",
        "\n",
        "# Full dataset references (use full MNIST everywhere except Kernel PCA sample)\n",
        "X_full = mnist.data\n",
        "y_full = mnist.target.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "NxgXOyCdwYHe",
        "outputId": "293a65d0-39c7-4600-c89b-9e96cea26941"
      },
      "outputs": [],
      "source": [
        "# below are drawings of some of the digits in the dataset\n",
        "mnist_names = [i for i in range(10)]\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(40):\n",
        "    plt.subplot(5, 8, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(np.array(mnist.data.iloc[i]).reshape((28, 28)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(mnist_names[int(mnist.target[i])])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFF-pENfwYHe"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a linear dimensionality reduction technique that identifies orthogonal axes of maximum variance. While computationally efficient, PCA may not capture complex non-linear relationships in the data. We will reduce the MNIST dataset to 2 components and examine the resulting visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "HkoUPjjSwYHe",
        "outputId": "7b8577d8-8e50-43ce-fb04-de7a4f4963b5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Fit PCA to 2 components and transform the data\n",
        "pca = PCA(n_components=2, whiten=True, random_state=42)\n",
        "mnist_pca = pca.fit_transform(mnist.data)\n",
        "\n",
        "# show the culmulative explained variance using 2 components\n",
        "print('Explained variance: ', np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# plot the data\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(mnist_pca[:, 0], mnist_pca[:, 1], c=mnist.target.astype(int), cmap='tab10', s=8)\n",
        "plt.colorbar()\n",
        "plt.xlabel('PCA First Component')\n",
        "plt.ylabel('PCA Second Component')\n",
        "plt.title('PCA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7LUsCtywYHf"
      },
      "source": [
        "## Kernel PCA\n",
        "\n",
        "Kernel PCA extends traditional PCA by applying the kernel trick to capture non-linear patterns in data. By using an RBF (Radial Basis Function) kernel, we can identify complex curved structures that linear PCA would miss.\n",
        "\n",
        "**Computational Considerations**: Kernel PCA has O(n²) to O(n³) complexity, making it expensive for large datasets. For this demonstration, we use a random sample of the MNIST data rather than all 70,000 points to maintain reasonable runtime while still illustrating the method's capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "eAxTVyu_wYHf",
        "outputId": "dac668b3-c442-4b87-afd9-1c483b224be0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Kernel PCA: use a smaller random sample due to computational cost\n",
        "from sklearn.decomposition import KernelPCA\n",
        "sample_size = 35000  # adjust for performance vs quality\n",
        "rng = np.random.RandomState(42)\n",
        "sample_idx = rng.choice(len(X_full), size=sample_size, replace=False)\n",
        "X_kpca = X_full.iloc[sample_idx]\n",
        "y_kpca = y_full[sample_idx]\n",
        "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.03)\n",
        "mnist_kpca = kpca.fit_transform(X_kpca)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(mnist_kpca[:, 0], mnist_kpca[:, 1], c=y_kpca, cmap='tab10', s=8)\n",
        "plt.colorbar()\n",
        "plt.xlabel('Kernel PCA First Component')\n",
        "plt.ylabel('Kernel PCA Second Component')\n",
        "plt.title(f'Kernel PCA (sample of {sample_size} points)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECBmFaSxwYHf"
      },
      "source": [
        "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE is a non-linear dimensionality reduction technique specifically designed for visualization. It excels at preserving local structure and revealing cluster patterns in high-dimensional data.\n",
        "\n",
        "**How t-SNE Works**:\n",
        "1. Converts high-dimensional Euclidean distances into conditional probabilities representing pairwise similarities\n",
        "2. Creates a similar probability distribution in the low-dimensional space\n",
        "3. Minimizes the divergence between these two distributions using gradient descent\n",
        "\n",
        "**Key Characteristics**:\n",
        "- **Strengths**: Excellent at revealing clusters and local neighborhoods; produces highly interpretable visualizations\n",
        "- **Limitations**: Can be computationally intensive on large datasets; primarily designed for visualization rather than general dimensionality reduction\n",
        "- **Performance Note**: Running t-SNE on the full MNIST dataset (70,000 points) takes considerable time with sklearn's implementation. For faster experimentation, consider using a random sample or exploring optimized implementations like openTSNE or MulticoreTSNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmfs2UzswYHf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# using t-SNE from sklearn on the entire dataset, it will take some time, feel free to take a random sample subset for your experiment\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "mnist_tsne = tsne.fit_transform(X_full)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "plt.colorbar()\n",
        "plt.xlabel('t-SNE First Component')\n",
        "plt.ylabel('t-SNE Second Component')\n",
        "plt.title('t-SNE (Full MNIST)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB_OXcNkwYHf"
      },
      "source": [
        "### Hyperparameter Tuning for t-SNE\n",
        "\n",
        "The quality of t-SNE visualizations depends heavily on hyperparameter selection. We will explore three key parameters:\n",
        "\n",
        "1. **Perplexity**: Controls the balance between local and global structure. Lower values (5-10) emphasize local patterns, while higher values (50-100) consider broader neighborhoods. Typical range: 5-50 for most datasets.\n",
        "\n",
        "2. **Initialization**: Starting point for optimization. PCA initialization typically provides more stable and interpretable results, while random initialization may reveal alternative structure.\n",
        "\n",
        "3. **Early Exaggeration**: Amplifies distances in early optimization to help form distinct clusters. Higher values create tighter, more separated clusters but may overemphasize divisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhVJO2TVwYHf"
      },
      "outputs": [],
      "source": [
        "# Perplexity sweep on full dataset (caution: very slow overall)\n",
        "perplexities = [5, 20, 50, 100]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, perplexity in enumerate(perplexities):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "    mnist_tsne = tsne.fit_transform(X_full)\n",
        "    plt.scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('t-SNE First Component')\n",
        "    plt.ylabel('t-SNE Second Component')\n",
        "    plt.title(f't-SNE perplexity {perplexity}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JERPYZkOwYHf"
      },
      "outputs": [],
      "source": [
        "# Initialization comparison (PCA vs random) on full dataset\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, init in enumerate(['pca', 'random']):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=50, init=init)\n",
        "    mnist_tsne = tsne.fit_transform(X_full)\n",
        "    plt.scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('t-SNE First Component')\n",
        "    plt.ylabel('t-SNE Second Component')\n",
        "    plt.title(f't-SNE init={init}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx30LHREwYHf"
      },
      "outputs": [],
      "source": [
        "# Early exaggeration sweep on full dataset\n",
        "early_exaggerations = [2, 5, 10, 50]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, early_exaggeration in enumerate(early_exaggerations):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    tsne = TSNE(n_components=2, random_state=42, early_exaggeration=early_exaggeration)\n",
        "    mnist_tsne = tsne.fit_transform(X_full)\n",
        "    plt.scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('t-SNE First Component')\n",
        "    plt.ylabel('t-SNE Second Component')\n",
        "    plt.title(f't-SNE early_exaggeration={early_exaggeration}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY2vladvwYHf"
      },
      "source": [
        "## Uniform Manifold Approximation and Projection (UMAP)\n",
        "\n",
        "UMAP is a modern dimensionality reduction technique that balances preserving both local and global structure. It constructs a high-dimensional graph representation of the data, then optimizes a low-dimensional layout. UMAP typically runs faster than t-SNE and often produces more meaningful global structure while maintaining cluster separation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "HFlDDNJvwYHf",
        "outputId": "531a1738-082e-4dc6-f134-0922db710a71"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# UMAP on full dataset\n",
        "import umap\n",
        "umap1 = umap.UMAP(n_components=2, random_state=42)\n",
        "mnist_umap = umap1.fit_transform(X_full)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(mnist_umap[:, 0], mnist_umap[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "plt.colorbar()\n",
        "plt.xlabel('UMAP First Component')\n",
        "plt.ylabel('UMAP Second Component')\n",
        "plt.title('UMAP (Full MNIST)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6NkQboAwYHg"
      },
      "source": [
        "### Hyperparameter Tuning for UMAP\n",
        "\n",
        "UMAP's behavior is controlled by several important hyperparameters. We will investigate three primary parameters:\n",
        "\n",
        "1. **n_neighbors**: Determines the size of the local neighborhood considered. Lower values (5-15) emphasize fine detail and local structure, while higher values (30-100) preserve more global structure. This is conceptually similar to perplexity in t-SNE.\n",
        "\n",
        "2. **min_dist**: Controls how tightly points are packed in the embedding. Lower values (0.0-0.1) create dense, tightly clustered visualizations, while higher values (0.5-0.99) produce more dispersed layouts with greater separation.\n",
        "\n",
        "3. **metric**: The distance function used to measure similarity in the original high-dimensional space. Different metrics capture different notions of similarity: Euclidean for overall magnitude, Manhattan for axis-aligned patterns, Cosine for directional similarity, and Correlation for pattern similarity regardless of scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "aweYITuEwYHg",
        "outputId": "68e044e3-41b0-4490-84c9-255907bbbad0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# UMAP n_neighbors sweep on full dataset (may be slow)\n",
        "n_neighbors = [5, 10, 20, 50]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, n_neighbor in enumerate(n_neighbors):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    umap2 = umap.UMAP(n_components=2, random_state=42, n_neighbors=n_neighbor)\n",
        "    mnist_umap = umap2.fit_transform(X_full)\n",
        "    plt.scatter(mnist_umap[:, 0], mnist_umap[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('UMAP First Component')\n",
        "    plt.ylabel('UMAP Second Component')\n",
        "    plt.title(f'UMAP n_neighbors={n_neighbor}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "CfxHmF7YwYHg",
        "outputId": "b7aec758-2903-44df-fb21-137846868d90"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# UMAP min_dist sweep on full dataset\n",
        "min_dists = [0.1, 0.25, 0.5, 0.99]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, min_dist in enumerate(min_dists):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    umap3 = umap.UMAP(n_components=2, random_state=42, min_dist=min_dist)\n",
        "    mnist_umap = umap3.fit_transform(X_full)\n",
        "    plt.scatter(mnist_umap[:, 0], mnist_umap[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('UMAP First Component')\n",
        "    plt.ylabel('UMAP Second Component')\n",
        "    plt.title(f'UMAP min_dist={min_dist}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al4bb4UMwYHg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# UMAP metric sweep on full dataset\n",
        "metrics = ['euclidean', 'manhattan', 'cosine', 'correlation']\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    umap4 = umap.UMAP(n_components=2, random_state=42, metric=metric)\n",
        "    mnist_umap = umap4.fit_transform(X_full)\n",
        "    plt.scatter(mnist_umap[:, 0], mnist_umap[:, 1], c=y_full, cmap='tab10', s=8)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('UMAP First Component')\n",
        "    plt.ylabel('UMAP Second Component')\n",
        "    plt.title(f'UMAP metric={metric}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmzEGidUwYHg"
      },
      "source": [
        "**Key takeaways:**\n",
        "\n",
        "t-SNE excels at revealing local cluster structure but can be computationally expensive. UMAP provides a good balance between computational efficiency and preservation of both local and global structure. The choice of method and hyperparameters should be guided by your specific goals and computational constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFzHGHbywYHg"
      },
      "source": [
        "## References\n",
        "\n",
        "This lab is adapted from the following resources:\n",
        "\n",
        "**t-SNE and UMAP Tutorials:**\n",
        "* Smendowski (2021). *Data Embedding and Visualization: MDS and t-SNE projections*  \n",
        "  GitHub: [Data Embedding Repository](https://github.com/Smendowski/data-embedding-and-visualization)\n",
        "\n",
        "* *Visualizing Neural Networks using t-SNE and UMAP*  \n",
        "  Kaggle: [Notebook Tutorial](https://www.kaggle.com/code/lzs0047/visualizing-neural-networks-using-t-sne-and-umap/)\n",
        "\n",
        "**Original Papers:**\n",
        "* van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE. *Journal of Machine Learning Research*, 9, 2579-2605.\n",
        "\n",
        "* McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. *arXiv preprint arXiv:1802.03426*.\n",
        "\n",
        "**Additional Resources:**\n",
        "* [UMAP Documentation](https://umap-learn.readthedocs.io/)\n",
        "* [scikit-learn Manifold Learning Guide](https://scikit-learn.org/stable/modules/manifold.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHmvPCw-wYHg"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stat362-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
