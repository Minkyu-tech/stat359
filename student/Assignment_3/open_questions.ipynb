{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b2a57",
   "metadata": {},
   "source": [
    "## Task 1: MLP with Mean-Pooled FastText Sentence Embedding (25 points)\n",
    "\n",
    "> Test F1 Macro: **0.7087** $\\ge$ 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9181f",
   "metadata": {},
   "source": [
    "## Task 2: LSTM with Padded FastText Word Vectors (25 points)\n",
    "\n",
    "> Test F1 Macro: **0.7404** $\\ge$ 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b8c88",
   "metadata": {},
   "source": [
    "## Evaluation Requirements (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47809c",
   "metadata": {},
   "source": [
    "### MLP\n",
    "<img src=\"outputs/mlp_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/mlp_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- Both the training and validation curves suggest the model learned well without clear signs of overfitting.\n",
    "- The confusion matrix indicates frequent misclassifications between the positive and neutral classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8748f",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "<img src=\"outputs/lstm_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/lstm_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- Training accuracy continues to increase, while validation performance starts to plateau around epoch ~20, suggesting mild overfitting.\n",
    "- Although confusion between positive and neutral remains, the LSTM achieves a higher F1 score than the MLP overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4624c",
   "metadata": {},
   "source": [
    "## Provided Models (Required) (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc546fb",
   "metadata": {},
   "source": [
    "### RNN\n",
    "<img src=\"outputs/rnn_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/rnn_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- Training performance keeps improving, but validation performance plateaus while validation loss rises sharply, indicating clear overfitting (Test Macro F1 ≈ 0.684).\n",
    "- The confusion matrix shows frequent positive → neutral errors, suggesting the model tends to absorb positive examples into the neutral class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc967b",
   "metadata": {},
   "source": [
    "### GRU\n",
    "<img src=\"outputs/gru_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/gru_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- Training metrics approach near-perfect scores, while validation loss increases later in training, suggesting overfitting, but the model generalizes better than the basic RNN (Test Macro F1 ≈ 0.749).\n",
    "- The main error pattern remains positive–neutral confusion, especially predicting positive samples as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde87d4c",
   "metadata": {},
   "source": [
    "### BERT\n",
    "<img src=\"outputs/bert_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/bert_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- During 5-epoch fine-tuning, validation F1 improves up to around epoch 3 and then fluctuates, implying there is a best checkpoint/epoch (Test Macro F1 ≈ 0.806).\n",
    "- The confusion matrix indicates more stable separation for neutral and negative, with remaining errors mainly between positive and neutral, but overall less confusion than classical baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfcc16",
   "metadata": {},
   "source": [
    "### GPT\n",
    "<img src=\"outputs/gpt_f1_learning_curves.png\" width=\"500\">\n",
    "<img src=\"outputs/gpt_confusion_matrix.png\" width=\"500\">\n",
    "\n",
    "- Validation F1 increases quickly and stabilizes within a few epochs, showing that a short fine-tuning schedule is sufficient (Test Macro F1 ≈ 0.787).\n",
    "- The confusion matrix still shows positive–neutral confusion as the dominant failure mode, while the neutral class tends to be predicted most reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab243a8",
   "metadata": {},
   "source": [
    "## Open-Ended Reflection Questions (23 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efa3a5",
   "metadata": {},
   "source": [
    "### 1. Training Dynamics\n",
    "Focus on your MLP and LSTM implementations\n",
    "\n",
    "> Did your models show signs of overfitting or underfitting? What architectural or training changes could address this?\n",
    "\n",
    "- For the MLP, the training and validation losses decrease together, and the validation F1 score also increases overall, so it seems that the model was trained stably without clear overfitting. However, for the LSTM, the training performance keeps increasing, but the validation performance becomes more gradual after about 20 epochs, so there is a possibility of overfitting. To improve this, we can adjust dropout or weight decay, adjust the learning rate to find a proper range, or select the optimal checkpoint using early stopping.\n",
    "\n",
    "> How did using class weights affect training stability and final performance?\n",
    "\n",
    "- The given dataset seems to have unbalanced classes, and if we do not use class weights, the model’s predictions can be biased toward the neutral class. This can lower the macro F1 score. By applying class weights with ```nn.CrossEntropyLoss(weight=...)```, a penalty is applied to the minor classes, so macro F1 can encourage more balanced training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ee699",
   "metadata": {},
   "source": [
    "### 2. Model Performance and Error Analysis\n",
    "Focus on your MLP and LSTM implementations\n",
    "\n",
    "> Which of your two models generalized better to the test set? Provide evidence from your metrics.\n",
    "\n",
    "- From [the Test Macro F1 results](#evaluation-requirements-10-points), the MLP is 0.7087 and the LSTM is 0.7404, so the LSTM seems to have generalized better to the test set with slightly higher scores. Also, when checking [the confusion matrices](#evaluation-requirements-10-points) above, we can see that the overall misclassifications of the LSTM are slightly reduced.\n",
    "\n",
    "> Which sentiment class was most frequently misclassified? Propose reasons for this pattern.\n",
    "\n",
    "- For both models, the most frequent misclassification happened between the positive and neutral classes. One reason is that the dataset itself has a large proportion of neutral samples, so the model’s predictions could have been biased toward neutral. Also, due to the characteristics of the MLP and LSTM models, they may have missed differences in nuance or important contextual clues. There were probably expressions with unclear boundaries as well, since this is financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cb946",
   "metadata": {},
   "source": [
    "### 3. Cross-Model Comparison\n",
    "Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT\n",
    "\n",
    "> How did mean-pooled FastText embeddings limit the MLP compared to sequence-based models?\n",
    "\n",
    "- Mean-pooling compresses the word vectors in a sentence into an average, so a lot of information such as word order, negation, and emphasis disappears. Therefore, because the MLP is already in a state where information is greatly reduced, it was difficult for it to use sentence structure and separate the boundary in a detailed way like sequence-based models.\n",
    "\n",
    "> What advantage did the LSTM’s sequential processing provide over the MLP?\n",
    "\n",
    "- The sequential processing of the LSTM has the advantage that it can reflect the relationship and flow of words before and after while processing the (32, 300) sequence in order. In other words, it can preserve important clues for sentiment such as negation, emphasis, and transitions in the sentence better, and as a result, we can see that the test macro F1 came out slightly higher than the MLP.\n",
    "\n",
    "> Did fine-tuned LLMs (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations.\n",
    "\n",
    "- As a result, BERT and GPT showed higher test macro F1 than the classical models. I think this is because it was possible to learn more efficiently by using pretrained weights, and because it caught that words in a sentence change depending on the context through contextual representations. In contrast, FastText-based models have relatively fixed and simplified word meanings, so they can easily miss the nuance of context.\n",
    "\n",
    "> Rank all six models by test performance. What architectural or representational factors explain the ranking?\n",
    "\n",
    "- By Test Macro F1 Scorce:\n",
    "    - 1st: BERT (0.8055)\n",
    "    - 2nd: GPT (0.7872)\n",
    "    - 3rd: GRU (0.7494)\n",
    "    - 4th: LSTM (0.7404)\n",
    "    - 5th: MLP (0.7087)\n",
    "    - 6th: RNN (0.6844)\n",
    "- BERT and GPT, which ranked at the top, could distinguish unclear boundaries like positive and neutral better thanks to pretraining and contextual representations.\n",
    "- GRU and LSTM, which are in the middle rank, could do sequencing, but it seems that effective learning would have been difficult due to the limited training data size and overfitting.\n",
    "- Among the lower rank models, the MLP had disadvantages due to mean-pooling as mentioned above, and in the case of the RNN, because there is no gate, the limitations would have appeared more strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "#### <font color=\"red\">Write your disclosure here.</font>\n",
    "- **Tool(s) used:** ChatGPT 5.2 Thinking <br>\n",
    "- **How you used them:** Used for concept explanations, asking example code, debuggings and correction of my responses. <br>\n",
    "- **What you verified yourself:** Reading documentation, refactoring the code, and interpreting the execution results. <br>\n",
    "- **What you did *not* use AI for (if applicable):** Code refactoring and logic comprehension. (Manually adapted and refactored the provided example code to fit my own coding style, understanding the implementation.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat359-su25-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
